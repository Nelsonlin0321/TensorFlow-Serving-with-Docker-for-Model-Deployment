{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"> Deploy Models with TensorFlow Serving and Docker</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ZQhSNW-CZUZh",
    "outputId": "0a0dd94e-b78f-4ffb-c819-5eae6efb6255"
   },
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Souce: https://www.kaggle.com/snap/amazon-fine-food-reviews/data\n",
    "# !head -n 2 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "def load_dataset(file_path,num_samples):\n",
    "    df = pd.read_csv(file_path,usecols = [6,9],nrows=num_samples)\n",
    "    \n",
    "    df.columns = ['rating','title']\n",
    "    \n",
    "    text = df['title'].to_list()\n",
    "    text = [str(t).encode('ascii','replace') for t in text]\n",
    "    text = np.array(text,dtype=object)[:]\n",
    "    \n",
    "    labels = df['rating'].tolist()\n",
    "    labels  = [1 if i>=4 else 0 if i==3 else -1 for i in labels]\n",
    "    labels = np.array(pd.get_dummies(labels),dtype=int)[:]\n",
    "    \n",
    "    return labels,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_lables,tmp_text = load_dataset('train.csv',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_lables[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Build the Classification Model using TF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "\n",
    "## https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\n",
    "## https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\n",
    "\n",
    "def get_model():\n",
    "    hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", \n",
    "                               output_shape=[50],input_shape=[], dtype=tf.string,\n",
    "                              trainable= False)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax',name=\"output\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer= \"Adam\",metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 50)                48190600  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                816       \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,191,467\n",
      "Trainable params: 867\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\")\n",
    "embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Define Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a train.py\n",
    "def train(EPOCHS=2, BATCH_SIZE=32, TRAIN_FILE=\"train.csv\",VAL_FILE='test.csv'):\n",
    "    WORKING_DIR = os.getcwd()\n",
    "    print(\"Loading training/validation data\")\n",
    "    y_train,x_train = load_dataset(TRAIN_FILE, num_samples=100000)\n",
    "    y_val, x_val = load_dataset(VAL_FILE, num_samples=10000)\n",
    "    print('Training the model ...')\n",
    "    model = get_model()\n",
    "    \n",
    "    model.fit(x_train, y_train, batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tf.keras.callbacks.ModelCheckpoint(os.path.join(WORKING_DIR,'model_checkpoint'),\n",
    "    monitor='val_loss',\n",
    "    verbose = 1,\n",
    "    save_best_model=True,\n",
    "    save_weights_only=False,\n",
    "    mode= 'auto' )])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Train and Export Model as Protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/validation data\n",
      "Training the model ...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_2 (KerasLayer)  (None, 50)                48190600  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                816       \n",
      "                                                                 \n",
      " output (Dense)              (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,191,467\n",
      "Trainable params: 867\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "3120/3125 [============================>.] - ETA: 0s - loss: 0.5926 - accuracy: 0.7850\n",
      "Epoch 1: saving model to E:\\MyFiles\\WorkSpace\\MLOps\\TensorFlow Serving with Docker for Model Deployment\\model_checkpoint\n",
      "INFO:tensorflow:Assets written to: E:\\MyFiles\\WorkSpace\\MLOps\\TensorFlow Serving with Docker for Model Deployment\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:\\MyFiles\\WorkSpace\\MLOps\\TensorFlow Serving with Docker for Model Deployment\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 15s 5ms/step - loss: 0.5925 - accuracy: 0.7850 - val_loss: 0.5702 - val_accuracy: 0.7878\n",
      "Epoch 2/2\n",
      "3110/3125 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.7903\n",
      "Epoch 2: saving model to E:\\MyFiles\\WorkSpace\\MLOps\\TensorFlow Serving with Docker for Model Deployment\\model_checkpoint\n",
      "INFO:tensorflow:Assets written to: E:\\MyFiles\\WorkSpace\\MLOps\\TensorFlow Serving with Docker for Model Deployment\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:\\MyFiles\\WorkSpace\\MLOps\\TensorFlow Serving with Docker for Model Deployment\\model_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 14s 4ms/step - loss: 0.5673 - accuracy: 0.7903 - val_loss: 0.5657 - val_accuracy: 0.7886\n",
      "INFO:tensorflow:Assets written to: amazon_review/1648864296\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: amazon_review/1648864296\\assets\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a train.py\n",
    "\n",
    "def export_model(model,base_path=\"amazon_review/\"):\n",
    "    path = os.path.join(base_path,str(int(time.time())))\n",
    "    tf.saved_model.save(model,path)\n",
    "    \n",
    "if __name__ == \"__main__\":      \n",
    "        model = train()\n",
    "        export_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43567666, 0.08142588, 0.4828974 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"horrible book, waste of time\"\n",
    "model.predict([test_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09453165, 0.03179249, 0.8736758 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"Awesome books\"\n",
    "model.predict([test_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : TensorFlow Serving with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command to pull the tensorflow/serving image\n",
    "```bash\n",
    "docker pull tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run -p 8500:8500 \\\n",
    "            -p 8501:8501 \\\n",
    "            --mount type=bind,\\\n",
    "            source=`pwd`/amazon_review/,\\ # docker 会寻找最新的文件下的model\n",
    "            target=/models/amazon_review \\\n",
    "            -e MODEL_NAME=amazon_review \\\n",
    "            -t tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过-p把docker的8501映射成Host的8501端口(而8500没有映射，因此8500的gRPC接口只能在Docker内部访问，而不能在Host以及其它机器上访问)\n",
    "\n",
    "- 同时我们把Host的路径/path/to/my_model/映射到docker里的/models/my_model，\n",
    "\n",
    "- 接着我们指定环境变量MODEL_NAME为my_model\n",
    "\n",
    "- 最后-t启动tensorflow/serving这个镜像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据上面的描述，我们把model_name设置成立amazon_review,而MODEL_BASE_PATH是默认的/models，因此tensorflow_model_server命令会去/models/amazon_review下寻找模型\n",
    "- 根据前面的–mount的bind，/models/amazon_review是Host机器的amazon_review/,也就是实际模型存放的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果我们想让外部可以通过gRPC访问，那么可以增加-p 8500:8500。当然我们不一定要使得Host的端口是8500，也可以是-p 12345:8500。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "rhyme@ip-10-199-92-231:~/Desktop/Project$ docker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/amazon_review/,target=/models/amazon_review -e MODEL_NAME=amazon_review -t tensorflow/serving\n",
    "2022-04-02 01:45:02.081597: I tensorflow_serving/model_servers/server.cc:89] Building single TensorFlow model file config:  model_name: amazon_review model_base_path: /models/amazon_review\n",
    "2022-04-02 01:45:02.081939: I tensorflow_serving/model_servers/server_core.cc:465] Adding/updating models.\n",
    "2022-04-02 01:45:02.081973: I tensorflow_serving/model_servers/server_core.cc:591]  (Re-)adding model: amazon_review\n",
    "2022-04-02 01:45:02.187726: I tensorflow_serving/core/basic_manager.cc:740] Successfully reserved resources to load servable {name: amazon_review version: 1648863436}\n",
    "2022-04-02 01:45:02.187780: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: amazon_review version: 1648863436}\n",
    "2022-04-02 01:45:02.187802: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: amazon_review version: 1648863436}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : Setup a REST Client to Perform Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Model Prediction\n",
    "\n",
    "##### Support for gRPC and REST\n",
    "\n",
    "- TensorFlow Serving supports\n",
    "    - Remote Procedure Protocal (gRPC)\n",
    "    - Representational State Transfer (REST)\n",
    "- Consistent API structures\n",
    "- Server supports both standards simultaneously\n",
    "- Default ports:\n",
    "    - RPC: 8500\n",
    "    - REST: 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions via REST\n",
    "\n",
    "- Standard HTTP POST requests\n",
    "- Response is a JSON body with the prediction\n",
    "- Request from the default or specific model\n",
    "\n",
    "Default URI scheme:\n",
    "\n",
    "`http://{HOST}:{PORT}/v1/models/{MODEL_NAME}`\n",
    "\n",
    "Specific model versions:\n",
    "\n",
    "`http://{HOST}:{PORT}/v1/models/{MODEL_NAME}[/versions/{MODEL_VERSION}]:predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NND75SMZUsP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_serving_rest_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_serving_rest_client.py\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "def get_rest_url(model_name, host='127.0.0.1', port='8501', verb='predict', version=None):\n",
    "    \"\"\" generate the URL path\"\"\"\n",
    "    url = \"http://{host}:{port}/v1/models/{model_name}\".format(host=host, port=port, model_name=model_name)\n",
    "    if version:\n",
    "        url += 'versions/{version}'.format(version=version)\n",
    "    url += ':{verb}'.format(verb=verb)\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_model_prediction(model_input, model_name='amazon_review', signature_name='serving_default'):\n",
    "    \"\"\" no error handling at all, just poc\"\"\"\n",
    "\n",
    "    url = get_rest_url(model_name)\n",
    "    #In the row format, inputs are keyed to instances key in the JSON request.\n",
    "    #When there is only one named input, specify the value of instances key to be the value of the input:\n",
    "    data = {\"instances\": [model_input]}\n",
    "    \n",
    "    rv = requests.post(url, data=json.dumps(data))\n",
    "    if rv.status_code != requests.codes.ok:\n",
    "        rv.raise_for_status()\n",
    "    \n",
    "    return rv.json()['predictions']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"\\nGenerate REST url ...\")\n",
    "    url = get_rest_url(model_name='amazon_review')\n",
    "    print(url)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nEnter an Amazon review [:q for Quit]\")\n",
    "        if sys.version_info[0] <= 3:\n",
    "            sentence = input()\n",
    "        if sentence == ':q':\n",
    "            break\n",
    "        model_input = sentence\n",
    "        model_prediction = get_model_prediction(model_input)\n",
    "        print(\"The model predicted ...\")\n",
    "        print(model_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "rhyme@ip-10-199-92-231:~/Desktop/Project$ python3 ./tf_serving_rest_client.py\n",
    "\n",
    "Generate REST url ...\n",
    "http://127.0.0.1:8501/v1/models/amazon_review:predict\n",
    "\n",
    "Enter an Amazon review [:q for Quit]\n",
    "I'm really enjoying reading this book!\n",
    "The model predicted ...\n",
    "[[0.0729031041, 0.0303271879, 0.896769702]]\n",
    "\n",
    "Enter an Amazon review [:q for Quit]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : Setup a gRPC Client to Perform Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from [https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py#L152)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions via gRPC\n",
    "\n",
    "More sophisticated client-server connections\n",
    "\n",
    "- Prediction data has to be converted to the Protobuf format\n",
    "- Request types have designated types, e.g. float, int, bytes\n",
    "- Payloads need to be converted to base64\n",
    "- Connect to the server via gRPC stubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gRPC vs REST: When to use which API standard\n",
    "\n",
    "- Rest is easy to implement and debug\n",
    "- RPC is more network efficient, smaller payloads\n",
    "- RPC can provide much faster inferences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKJVOjDlZUvc"
   },
   "outputs": [],
   "source": [
    "%%writefile tf_serving_grpc_client.py\n",
    "import sys\n",
    "import grpc\n",
    "from grpc.beta import implementations\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2, get_model_metadata_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "def get_stub(host='127.0.0.1', port='8500'):\n",
    "    \"\"\"创建stub连接器\"\"\"\n",
    "    \"\"\"\n",
    "    A module acting as the interface for gRPC client.\n",
    "\n",
    "    You can do everything in the client side via GRPC.Stub, \n",
    "    including connecting, sending/receiving steaming or non-steaming requests, canceling calls\n",
    "    and so on.\n",
    "    \"\"\"\n",
    "    channel = grpc.insecure_channel(f'{host}:{port}') \n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    return stub\n",
    "\n",
    "\n",
    "def get_model_prediction(model_input, stub, model_name='amazon_review', signature_name='serving_default'):\n",
    "    \"\"\" no error handling at all, just poc\"\"\"\n",
    "    \n",
    "    \"\"\"创建grpc request 对象\"\"\"\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    \n",
    "    \"\"\"编辑request属性\"\"\"\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = signature_name\n",
    "    request.inputs['input_input'].CopyFrom(tf.make_tensor_proto(model_input))\n",
    "    \n",
    "    \"\"\"将request 对象传给 stub 进行预测\"\"\"\n",
    "    response = stub.Predict.future(request, 5.0)  # 5 seconds\n",
    "    return response.result().outputs[\"output\"].float_val\n",
    "\n",
    "\n",
    "def get_model_version(model_name, stub):\n",
    "    request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "    request.model_spec.name = 'amazon_review'\n",
    "    request.metadata_field.append(\"signature_def\")\n",
    "    response = stub.GetModelMetadata(request, 10)\n",
    "    # signature of loaded model is available here: response.metadata['signature_def']\n",
    "    return response.model_spec.version.value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\nCreate RPC connection ...\")\n",
    "    stub = get_stub()\n",
    "    while True:\n",
    "        print(\"\\nEnter an Amazon review [:q for Quit]\")\n",
    "        if sys.version_info[0] <= 3:\n",
    "            sentence = raw_input() if sys.version_info[0] < 3 else input()\n",
    "        if sentence == ':q':\n",
    "            break\n",
    "        model_input = [sentence]\n",
    "        model_prediction = get_model_prediction(model_input, stub)\n",
    "        print(\"The model predicted ...\")\n",
    "        print(model_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TidfRe2VZU39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xA3wNmDSZU66"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deploy-TF-Serving.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
